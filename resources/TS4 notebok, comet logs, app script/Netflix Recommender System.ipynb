{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.stonesincuba.com/wp-content/uploads/2020/06/Best-Movies-On-Netflix.jpg\" style=\"width:500px;height:600px:\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objective-of-this-notebook\" data-toc-modified-id=\"Objective-of-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objective of this notebook</a></span></li><li><span><a href=\"#Problem-we-seek-to-address\" data-toc-modified-id=\"Problem-we-seek-to-address-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem we seek to address</a></span></li><li><span><a href=\"#Datasources-we-seek-to-base-our-models-on\" data-toc-modified-id=\"Datasources-we-seek-to-base-our-models-on-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Datasources we seek to base our models on</a></span></li></ul></li><li><span><a href=\"#Importing-the-data-and-the-necessary-packages\" data-toc-modified-id=\"Importing-the-data-and-the-necessary-packages-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing the data and the necessary packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Installing-and-importing-necessary-Python-packages\" data-toc-modified-id=\"Installing-and-importing-necessary-Python-packages-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Installing and importing necessary Python packages</a></span></li><li><span><a href=\"#Importing-the-data-into-dataframes\" data-toc-modified-id=\"Importing-the-data-into-dataframes-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data into dataframes</a></span></li></ul></li><li><span><a href=\"#Initializing-comet-for-version-control\" data-toc-modified-id=\"Initializing-comet-for-version-control-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Initializing comet for version control</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removing-unnecessary-columns\" data-toc-modified-id=\"Removing-unnecessary-columns-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Removing unnecessary columns</a></span></li><li><span><a href=\"#Quantifying-the-genres\" data-toc-modified-id=\"Quantifying-the-genres-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Quantifying the genres</a></span></li><li><span><a href=\"#Creating-genre-and-tag-metadata-for-each-movie\" data-toc-modified-id=\"Creating-genre-and-tag-metadata-for-each-movie-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Creating genre and tag metadata for each movie</a></span></li></ul></li><li><span><a href=\"#Exploring-the-data\" data-toc-modified-id=\"Exploring-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Exploring the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset-descriptions\" data-toc-modified-id=\"Dataset-descriptions-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><strong>Dataset descriptions</strong></a></span></li><li><span><a href=\"#Overview-of-all-the-datasets\" data-toc-modified-id=\"Overview-of-all-the-datasets-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Overview of all the datasets</a></span></li><li><span><a href=\"#Displaying-the-heads-of-the-tables\" data-toc-modified-id=\"Displaying-the-heads-of-the-tables-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Displaying the heads of the tables</a></span></li><li><span><a href=\"#Checking-for-null-values,-shapes-and-datatypes\" data-toc-modified-id=\"Checking-for-null-values,-shapes-and-datatypes-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Checking for null-values, shapes and datatypes</a></span></li><li><span><a href=\"#Most-common-genres\" data-toc-modified-id=\"Most-common-genres-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Most common genres</a></span></li><li><span><a href=\"#Ratings-distribution\" data-toc-modified-id=\"Ratings-distribution-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Ratings distribution</a></span></li><li><span><a href=\"#Insights-on-the-directors\" data-toc-modified-id=\"Insights-on-the-directors-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Insights on the directors</a></span></li><li><span><a href=\"#Most-Rated-Movies\" data-toc-modified-id=\"Most-Rated-Movies-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Most Rated Movies</a></span></li><li><span><a href=\"#Displaying-the-amount-of-unique-values\" data-toc-modified-id=\"Displaying-the-amount-of-unique-values-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Displaying the amount of unique values</a></span></li><li><span><a href=\"#Best-and-worst-rated-movies\" data-toc-modified-id=\"Best-and-worst-rated-movies-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Best and worst rated movies</a></span></li><li><span><a href=\"#Which-genres-have-the-highest-average-rating?\" data-toc-modified-id=\"Which-genres-have-the-highest-average-rating?-5.11\"><span class=\"toc-item-num\">5.11&nbsp;&nbsp;</span>Which genres have the highest average rating?</a></span></li><li><span><a href=\"#Which-movie-has-the-most-polarising-effect-on-the-audience?\" data-toc-modified-id=\"Which-movie-has-the-most-polarising-effect-on-the-audience?-5.12\"><span class=\"toc-item-num\">5.12&nbsp;&nbsp;</span>Which movie has the most polarising effect on the audience?</a></span></li></ul></li><li><span><a href=\"#Building-and-training-a-content-based-model\" data-toc-modified-id=\"Building-and-training-a-content-based-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Building and training a content based model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-data\" data-toc-modified-id=\"Splitting-the-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Splitting the data</a></span></li><li><span><a href=\"#Building-and-training-the-model\" data-toc-modified-id=\"Building-and-training-the-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Building and training the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weighing-the-significance-of-words-with-TfirdfVectorizer\" data-toc-modified-id=\"Weighing-the-significance-of-words-with-TfirdfVectorizer-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Weighing the significance of words with TfirdfVectorizer</a></span></li><li><span><a href=\"#Building-a-similarity-matrix-with-cosine_similarity\" data-toc-modified-id=\"Building-a-similarity-matrix-with-cosine_similarity-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Building a similarity matrix with cosine_similarity</a></span></li><li><span><a href=\"#Recommending-movies\" data-toc-modified-id=\"Recommending-movies-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Recommending movies</a></span></li><li><span><a href=\"#Predicting-movie-ratings-for-users\" data-toc-modified-id=\"Predicting-movie-ratings-for-users-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Predicting movie ratings for users</a></span></li></ul></li></ul></li><li><span><a href=\"#Building-and-training-collaborative-based-models\" data-toc-modified-id=\"Building-and-training-collaborative-based-models-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Building and training collaborative based models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Text-Preprocessing\" data-toc-modified-id=\"Text-Preprocessing-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Text Preprocessing</a></span></li><li><span><a href=\"#The-SVD-model\" data-toc-modified-id=\"The-SVD-model-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>The SVD model</a></span></li><li><span><a href=\"#The-CoClustering-model\" data-toc-modified-id=\"The-CoClustering-model-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>The CoClustering model</a></span></li><li><span><a href=\"#The-KNNBaseine-model\" data-toc-modified-id=\"The-KNNBaseine-model-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>The KNNBaseine model</a></span></li><li><span><a href=\"#The-Hypertuned-SVD-model\" data-toc-modified-id=\"The-Hypertuned-SVD-model-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>The Hypertuned SVD model</a></span></li></ul></li><li><span><a href=\"#Exporting-predictions-to-Kaggle\" data-toc-modified-id=\"Exporting-predictions-to-Kaggle-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Exporting predictions to Kaggle</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Reference-list\" data-toc-modified-id=\"Reference-list-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Reference list</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective with this notebook is to build different movie recommender algorithms which can accurately predict how a user will rate a movie based on their historical preferences. The algorithms will make use of collaborative filtering as well as content based filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forms part of a public Kaggle competition and will be evaluated by the best RMSE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem we seek to address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewers of movies want to watch movies that they will like. This will increase their satisfaction and will make them more loyal to the Netflix platform which will extend the duration of their subscription, and cause them to recommend this platform to others over against the platforms of their competition (like Showmax). This will result in more loyal customers which will result in greater income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasources we seek to base our models on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens has gathered millions of movie ratings together with the movieId, UserId, and timestamp when the movie was rated. We also have the following information available on each movie from other public sources: 1. Movie-titles, 2. Movie-genres, 3. Title-cast, 4. Movie-budget, 5. Plot-keywords, 6. Director, 7. Runtime, 8. genome-scores.\n",
    "\n",
    "We will seek to use all of the data above to train our unsupervised machine learning models on to build the best recommender system within our abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data and the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump in, it is necessary to install and import some python packages that will help us in our quest. We are indebted to those who have contributed to building these packages and making them freely available. Note that there might be some packages that you first need to install on your local machine before it will run. Some of the common ones we have commented below and you can just remove the hashtag to install them on your local machine and then comment it out again after it has been installed on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and importing necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the section below to install packages that might not have been preinstalled on your local machine.\n",
    "\n",
    "# !conda install -c conda-forge/label/cf202003 scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importinng package for version control\n",
    "from comet_ml import Experiment\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing package for data visualization and analytics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Importing packages for modeling\n",
    "import heapq\n",
    "from surprise import CoClustering\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise import KNNBaseline\n",
    "\n",
    "# Packages for model evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from time import time\n",
    "\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Package to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Packages for saving models\n",
    "import pickle\n",
    "\n",
    "# Tweaking some of the packages for our use\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the pandas read_csv function to import all of the csv files into dataframes that we can manipulate and fit our models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/edsa-movie-recommendation-challenge/movies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1aff607d31f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading in the csv files and storing them as pandas DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_movies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/kaggle/input/edsa-movie-recommendation-challenge/movies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_imdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imdb_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_genome_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'genome_tags.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://github.com/Explore-AI/unsupervised-predict-streamlit-template/tree/master/resources/data/ratings.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Atom\\Anacondas\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Atom\\Anacondas\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Atom\\Anacondas\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Atom\\Anacondas\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Atom\\Anacondas\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/edsa-movie-recommendation-challenge/movies.csv'"
     ]
    }
   ],
   "source": [
    "# Reading in the csv files and storing them as pandas DataFrames\n",
    "df_movies = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/movies.csv')\n",
    "df_imdb = pd.read_csv('imdb_data.csv')\n",
    "df_genome_tags = pd.read_csv('genome_tags.csv')\n",
    "df_train = pd.read_csv('https://github.com/Explore-AI/unsupervised-predict-streamlit-template/tree/master/resources/data/ratings.csv')\n",
    "df_tags = pd.read_csv('tags.csv')\n",
    "df_links = pd.read_csv('links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing comet for version control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comet is a Python package developed for version control. We initialize the package by calling the Experiment model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up comet experiment\n",
    "experiment = Experiment(api_key='tkurqVVQirb76FiYYpbvSOgrz', project_name= 'recommender-system-1' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have such a large dataset we will seek to drop all of the redundant columns from the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by dropping the timestamps because it does not add value to training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the timestamp column from df_tags and df_train dataframes.\n",
    "df_tags.drop(['timestamp'],1, inplace=True)\n",
    "df_train.drop(['timestamp'],1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genres in the df_movies DataFrame is a helpful datasource that help us train a content-based recommender model. But in order to make use of the data we first need to split the genres by delimter and place them into lists next to each movie. Then we will have to use the list to indicate which movies contain which genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the delimited genre string into a list of genres\n",
    "\n",
    "df_movies['genre_split'] = df_movies['genres'].apply(lambda x: x.split('|'))\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at all the unique genres in the dataset\n",
    "\n",
    "# Creating a list of genre lists\n",
    "genres = list(df_movies['genre_split']) \n",
    "\n",
    "# Flattening the list of lists into a single list with duplicates\n",
    "genres = [item for items in genres for item in items] \n",
    "\n",
    "# Converting it to a set to remove duplicates, then back to a list\n",
    "genres = list(set(genres)) \n",
    "\n",
    "# Sorting the lis into alphabetical order ane displaying it\n",
    "genres.sort() \n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying the genre list for each movie\n",
    "\n",
    "# Adding the new columns and initially filling them with zeros\n",
    "for genre in genres:\n",
    "    df_movies[genre] = 0   \n",
    "\n",
    "# Splitting the genre list from each row\n",
    "def genre_1hot(row):\n",
    "    genre_list = row['genre_split']    \n",
    "    \n",
    "    # for each genre that is in the list, make the respective column of the row equal to 1\n",
    "    for genre in genre_list:   \n",
    "        row[genre] = 1    \n",
    "    return row\n",
    "\n",
    "# Applying the function to all rows\n",
    "df_movies = df_movies.apply(genre_1hot, axis=1)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with the year as another piece of data that our content based recommender can use.\n",
    "\n",
    "df_movies['year'] = df_movies['title'].apply(lambda x: x.split('(')[-1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the columns so that year is next to title instead of at the end\n",
    "cols = df_movies.columns.tolist()\n",
    "cols = cols[:2] + cols[-1:] + cols [2:-1]\n",
    "df_movies = df_movies[cols]\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating genre and tag metadata for each movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create metadata for each movie to be used in the content based recommender system. This will consist out of the genres and the tags. We will do this by firstly merging the tags unto df_movies. Then we will group the tags by MovieId and finally we will combine the genres and the tags into one cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the df_movies and df_tags dataframes.\n",
    "df_mixed = pd.merge(df_movies, df_tags, on='movieId', how='left')\n",
    "df_mixed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the tags by the movieId\n",
    "df_mixed.fillna(\"\", inplace=True)\n",
    "df_mixed = pd.DataFrame(df_mixed.groupby('movieId')['tag'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "df_final = pd.merge(df_movies, df_mixed, on='movieId', how='left')\n",
    "\n",
    "# Concatenating the tags and the genres as our metadata\n",
    "df_final['metadata'] = df_final[['tag', 'genres']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "df_final[['movieId','title','year', 'metadata']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can build our models it is important to get a basic understanding of the data we are working with so that we can pre-process the data accurately and choose the right models. In this section we will display and draw insights from the data we are working with by using both visual and non-visual display methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset descriptions**\n",
    "\n",
    "The supplied dataset comprises the following:\n",
    "\n",
    "1. **genome_scores.csv** - A score mapping the strength between movies and tag-related properties\n",
    "2. **train.csv** - The training split of the dataset. Contains user and movie IDs with associated rating data\n",
    "3. **test.csv** - The test split of the dataset. Contains user and movie IDs with no rating data\n",
    "4. **tags.csv** - User assigned for the movies within the dataset\n",
    "5. **links.csv** - File providing a mapping between a movie ID, IMDB IDs and TMDB IDs\n",
    "6. **movies** - File providing details about the title of the movie, genres and movieID that further can be used to merge to other related datasets\n",
    "7. **imdb_data.csv** - Additional movie metadata scraped from IMDB using the links.csv file\n",
    "8. **genome_tags.csv** - User assigned tags for genome-related scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of all the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get an overview of the amount of rows and columns in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list that contains the names of all the dataframes \n",
    "dfs = [df_train, df_genome_tags, df_imdb, df_links, df_movies, df_tags]\n",
    "\n",
    "# Create a list of the names of the imported datasets\n",
    "df_names = ['train', 'test', 'genome_tags',\n",
    "            'imdb_data', 'links', 'movies', 'tags']\n",
    "\n",
    "# Creating an empty dictionary\n",
    "dfs_dict = {}  \n",
    "\n",
    "# Iterating over the list and dictionary\n",
    "for name, data in zip(df_names, dfs):  \n",
    "    dfs_dict[name] = [data.shape[0], data.shape[1]]\n",
    "    df_prop = pd.DataFrame(dfs_dict,index=['rows', 'columns']).transpose()\n",
    "df_entries_count = df_prop.sort_values(by='rows', ascending=False)\n",
    "\n",
    "# Viewing the final output\n",
    "df_entries_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the overview of the amount of rows and column in each DataFrame that the genome_scores and the train datasets are especially big and might need to be reduced to reduce runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the heads of the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now show the first 5 rows in the four main datasets to get a quick idea of the data in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Viewing the train data to see the data present in the table\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that this dataset contains the ratings that specific users gave to specific movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally this dataset contained the movie title, release year and genres. After feature engineering each genre is added to a list and quantified for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tags.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset shows the user assigned tags to movies. It appears that users are not required to assign any tag and were allowed to assign more than one tag to each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imdb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset appears to contain the title_cast with the most famous actors mentioned first, the director, runtime, movie budget and plot keywords for each movie. This can all be very useful in building the content based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for null-values, shapes and datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we test for null values as well as look at the shape of the dataframes and the value types it contain. This will help us understand if we should fill null values as well as establish which datatypes we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the inforation of the train data, the object types and the total number of columns.\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train data set has 10,000,038 entries. That's a lot of data. It has 4 columns consisting of 1 float type data and 3 integer type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the train dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since genres is such an important piece of data for a content based recommender, it is helpful to see which genres occur the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe containing only the movieId and genres\n",
    "movies_genres = pd.DataFrame(df_movies[['movieId', 'genres']],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "# Splitting the genres seperated by \"|\" and create a list containing the genres allocated to each movie\n",
    "movies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n",
    "\n",
    "# Creating an expanded dataframe where each movie-genre combination is in a seperate row\n",
    "movies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "movies_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the genres from most common to least common\n",
    "plot = plt.figure(figsize=(15, 10))\n",
    "plt.title('Most common genres\\n', fontsize=20)\n",
    "sns.countplot(y=\"genres\", data=movies_genres,\n",
    "              order=movies_genres['genres'].value_counts(ascending=False).index,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top five most common movie genres are Drama, Comedy, Thriller, Romance and Action. This suggests that the most watched movies in this dataframe belong to these top five genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an idea of how the ratings are distributed by plotting a vertical bar chart of the counts of each rating. In order to do this we will first have to merge some tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining both train and movies datasets by using movieId\n",
    "# as the matching column between both datasets\n",
    "train_movies_df = pd.merge(df_train,\n",
    "                           df_movies,\n",
    "                           how='left',\n",
    "                           on='movieId')\n",
    "\n",
    "# Combining all the observations in movies_metadata_df with imdb_data\n",
    "# Using movieId as the matching column between both dataframes\n",
    "movies_imdb_df = pd.merge(train_movies_df,\n",
    "                              df_imdb,\n",
    "                              how='left',\n",
    "                              on='movieId')\n",
    "\n",
    "movies_imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average movie score rating in our data\n",
    "avg_rating = np.mean(movies_imdb_df[\"rating\"])\n",
    "\n",
    "print(f'Average rating score in the movies rating distribution is : {avg_rating} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the ratings distrubution of the movies in our dataset\n",
    "with sns.axes_style('darkgrid'):\n",
    "    g = sns.factorplot(\"rating\", data=movies_imdb_df, aspect=2.5, kind='count')\n",
    "    g.set_ylabels(\"Total number of ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the ratings fall in the 4.0 bucket, these suggest that most of the movies found in this dataset are rated slightly above the average rating by the viewers. Not that many movies have a rating below 2, the viewers most likely enjoy the majority of the movies in this dataset or are just prone to rate higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on the directors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many viewers choose movies based on directors and often directors would make similar type of movies. Thus if a viewer enjoyed one movie, he/she would likely enjoy the director's other movies. This would aid in our content based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directors with the most ratings in the dataset\n",
    "most_rated_director = pd.DataFrame(movies_imdb_df.groupby('director')['rating'].mean().\n",
    "                             sort_values(ascending=False))\n",
    "most_rated_director['No_of_ratings'] = movies_imdb_df.groupby('director')['rating'].count()\n",
    "most_rated_director.sort_values(by=['No_of_ratings', 'rating'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The director with the most ratings is Quentin Tarantino, this does not come as a surprise as this director has several blockbuster movies that performed quite well in box office. This director has directed movies such as Pulp Fiction and Kill Bill just to name two. J.R.R. Tolkien, Steven King and Jonathan Nolan have an average rating of 4.0, which is a higher average than all the other directors in our data set.\n",
    "\n",
    "Both Tolkien and King have very successful book franchises that we later adapted into movies that got a mostly positive reception from the public. Their ratings could be coming from people who have read the books and went on to watch the movies and those who just watched the movies alone, this doubles the total number of views the movies received. The movies are not watched by movie fans alone but also by book readers. They capture a wider audience.\n",
    "\n",
    "Johathan Nolan has directed movies such as Batman: The Dark Knight and The Dark Knight Rises which performed well with box office bringing in revenues that far outweighed the movie budgets and received positive ratings from movie critics and fans alike. It seems that the movie viewers in our dataset enjoy watching his movies as he has a proven track record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Rated Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see which movies were rated most by drawing a horizontal bar chart of the 10 most often rated movies of all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregated table of movies with their average ratings\n",
    "\n",
    "avg_ratings = df_train[['movieId','rating']].groupby(by='movieId', as_index=False).mean()\n",
    "\n",
    "# add a column that shows how many ratings each movie got\n",
    "avg_ratings['movie_count'] = df_train[['movieId','rating']].groupby(by='movieId', as_index=False).count()['rating']\n",
    "avg_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_ratings(series):\n",
    "    \"\"\"\n",
    "    This function generates a bar plot of the 10 most rated movies\n",
    "    of all time\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    data = series.value_counts().head(10)\n",
    "    plt.figure(figsize = (14,9))\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.barplot(x = data, y = data.index ,order = data.index, \n",
    "                palette = 'rocket', orient = 'h',edgecolor = \"black\")\n",
    "    plt.ylabel('Movies', fontsize = 15)\n",
    "    plt.xlabel('Number of ratings',fontsize = 15)\n",
    "    plt.title('10 most rated movies of all time',fontsize = 18)\n",
    "    plt.show()\n",
    "\n",
    "movie_ratings(movies_imdb_df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shawshank Redemption is the most rated movie of all time, one of the contributing factors could be that Morgan Freeman is one of the cast members. The actor has amassed a wealth of respect from movie fans and creators alike. The book that the movie was adopted from was written by Stephen King, a well known writer whose works have been adapted into several movies in Hollywood. Forest Gump and Pulp Fiction follow in 2nd and 3rd place respectively. These two movies also cast all star actors such as Tom Hanks, Samuel L. Jackson and John Travolta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the count of ratings per user\n",
    "df_ratingcount = df_train.groupby('userId').count().drop('movieId', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the ratings per user in a histogram with a filter at 600 to not display outliers.   \n",
    "plt.figure(figsize = (14,9))\n",
    "sns.histplot(x = df_ratingcount[df_ratingcount['rating']<600]['rating'], data = df_train['userId'].count(), \n",
    "                edgecolor = \"black\", bins = 5)\n",
    "plt.ylabel('Number of Users', fontsize = 15)\n",
    "plt.xlabel('Amount of Ratings',fontsize = 15)\n",
    "plt.title('Number of Ratings per User',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that the amount of ratings users give are skew to the right. As many as 140,000 users give less than 100 movies ratings, while less than 20,000 users give more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the amount of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the amount of unique users and movies and compare it with the highest userId and movieId to see if they are in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_users=(df_train.userId.nunique()) # number of unique reviewers\n",
    "n_movies=(df_train.movieId.nunique()) # number of unique movies\n",
    "max_userId = df_train.userId.max() # maximum value of userID\n",
    "max_movieId = df_train.movieId.max() # maximum value of movieId\n",
    "\n",
    "\n",
    "print('The highest userID is ' + str(max_userId))\n",
    "print('There are ' + str(n_users) + ' unique users.')\n",
    "\n",
    "print('The highest movieID is ' + str(max_movieId))\n",
    "print('There are ' + str(n_movies) + ' unique movies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the number of unique users matches the highest userId showing that they are in sync.\n",
    "We also observe that the highest movieId is much higher than the number of unique movies. This means that the movieId's are not consecutive numbers, and there are some movies without any ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best and worst rated movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the top 5 and bottom 5 ranked videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an aggregated table of movies with their average ratings\n",
    "avg_ratings = df_train[['movieId','rating']].groupby(by='movieId', as_index=False).mean()\n",
    "\n",
    "# Adding a column that shows how many ratings each movie got\n",
    "avg_ratings['movie_count'] = df_train[['movieId','rating']].groupby(by='movieId', as_index=False).count()['rating']\n",
    "\n",
    "# Merging avg_ratings with the movies dataframe so that we get the movie title for each movieId\n",
    "avg_ratings_merged = pd.merge(avg_ratings, df_movies[['movieId','title']], how='left', on='movieId')\n",
    "avg_ratings_merged = avg_ratings_merged[['title', 'rating', 'movie_count']]\n",
    "avg_ratings_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 Best rated movies are: \\n')\n",
    "print(avg_ratings_merged[['title', 'rating', 'movie_count']].sort_values(by='rating', ascending=False).head(5))\n",
    "print('5 Worst rated movies are: \\n')\n",
    "print(avg_ratings_merged[['title', 'rating', 'movie_count']].sort_values(by='rating', ascending=False).tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the top 5 and bottom 5 movies have only had 1 review each, so the average isn't a fair representation. What happens if we limit the above ranking to movies that have at least 50 reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "familiar_movies = avg_ratings_merged[avg_ratings_merged['movie_count'] >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('5 Best rated movies are: \\n')\n",
    "print(familiar_movies[['title','rating','movie_count']].sort_values(by='rating', ascending=False).head(5))\n",
    "print('')\n",
    "print('5 Worst rated movies are: \\n')\n",
    "print(familiar_movies[['title','rating','movie_count']].sort_values(by='rating', ascending=False).tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = familiar_movies[['title', 'rating']].sort_values(by = 'rating', ascending=False).head()['rating']\n",
    "y = familiar_movies[['title', 'rating']].sort_values(by = 'rating', ascending=False).head()['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,9))\n",
    "sns.set_style('white')\n",
    "sns.barplot(x = x, y = y, palette = 'mako', edgecolor = \"black\")\n",
    "plt.ylabel('Movie Names', fontsize = 15)\n",
    "plt.xlabel('Movie Ratings',fontsize = 15)\n",
    "plt.title('The 5 best rated movies with more than 50 ratings',fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = familiar_movies[['title', 'rating']].sort_values(by = 'rating', ascending=True).head()['rating']\n",
    "y = familiar_movies[['title', 'rating']].sort_values(by = 'rating', ascending=True).head()['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,9))\n",
    "sns.set_style('white')\n",
    "sns.barplot(x = x, y = y, palette = 'rocket', edgecolor = \"black\")\n",
    "plt.ylabel('Movie Names', fontsize = 15)\n",
    "plt.xlabel('Movie Ratings',fontsize = 15)\n",
    "plt.title('The 5 worst rated movies with more than 50 ratings', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. Planet Earth II and Planet Earth are two very good quality documentaries narrated by BBC senior manager David Attenborough. We will also see that documentaries overall tend to get better ratings than films from other genres. Shawshank Redemption performed the best for a movie rated that many times. One of the contributing factors could be that Morgan Freeman is one of the cast members. The actor has amassed a wealth of respect from movie fans and creators alike. The book that the movie was adopted from was written by Stephen King, a well known writer whose works have been adapted into several movies in Hollywood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this with sources like IMDb and Rotten Tomatoes we can see that Planet Earth has a 9.4/10 average rating and SuperBabies: Baby Geniuses 2 has an average rating of 2.0/10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Which genres have the highest average rating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at the average rating per genre and make a vertical barchart from it so that it is easy to see how many viewers on average enjoyed each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the average rating and rating count to the movies dataframe\n",
    "df_movies = pd.merge(df_movies, avg_ratings, how='left', on='movieId')\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the 'no genres' column because it contains brackets\n",
    "df_movies.rename(columns={'(no genres listed)':'No genre'}, inplace=True)\n",
    "\n",
    "# Renaming the first element of our list of genres\n",
    "genres[0] = 'No genre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dictionary to store averages in\n",
    "\n",
    "genre_averages ={}\n",
    "\n",
    "for genre in genres:\n",
    "    # Creating a temporary matrix for each genre that has 2 rows\n",
    "    # When the genre is 1, and when it is 0, calculating the mean for each\n",
    "    \n",
    "    temp = df_movies[[genre,'rating']].groupby(by=genre, as_index=False).mean()\n",
    "    \n",
    "    # Adding the value for when the genre is 1 to the dictionary\n",
    "    genre_averages[genre] = temp[temp[genre] == 1]['rating'].values[0]\n",
    "    \n",
    "genre_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vertical barchart to see which genres are rated highest on average\n",
    "\n",
    "plt.figure(figsize = (14,9))\n",
    "sns.barplot(x=list(genre_averages.keys()), y=list(genre_averages.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Average rating for each genre')\n",
    "plt.ylabel('Average rating')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylim((2,4))\n",
    "plt.axhline(y=np.median(list(genre_averages.values())), label='Median = ' + str(round(np.median(list(genre_averages.values())),2)))\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=(1.02, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above plot that the median average genre rating is 3.08. Documentaries have the highest average ratings (3.5) and Horror movies have by far the lowest average ratings (2.5).\n",
    "\n",
    "It is worth noting that each movie is weighted the same in this calculation, so a movie with 1 review has the same weight as a movie with 10,000 reviews. Also many movies have multiple genres and are included in the calculation for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which movie has the most polarising effect on the audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the movie with the greatest range of score by looking for the movie with the highest standard deviation. We will then plot the ratings of that movie to verify our suspicions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby movieId again but this time calculate the standard deviations\n",
    "\n",
    "deviations = df_train[['movieId','rating']].groupby(by='movieId', as_index=True).std()\n",
    "deviations['movie_count'] = df_train[['movieId','rating']].groupby(by='movieId', as_index=True).count()['rating']\n",
    "deviations.rename(columns={'rating':'std'}, inplace=True)\n",
    "deviations.sort_values(by='std', ascending=False, inplace=True)\n",
    "# again limit to movies with at least 50 ratings\n",
    "deviations[deviations['movie_count'] >= 50].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the movieId of the movie with the highest standard deviation\n",
    "\n",
    "largest_std = deviations[deviations['movie_count'] >= 50].iloc[0].name\n",
    "print(df_movies[df_movies['movieId']==largest_std]['title'].values[0] + ' has:')\n",
    "print('Average rating of ' + str(avg_ratings[avg_ratings['movieId']==largest_std]['rating'].values[0]))\n",
    "print('Deviation of ' + str(deviations.loc[largest_std]['std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Santa with muscles\" has an average rating more or less in the middle of the scale and the standard deviation that is equal to a large proportion of the distance from the mean to the maximum and minimum possible scores. Let's take a closer look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ratings of Santa with Muscles\n",
    "plt.figure(figsize = (14,9))\n",
    "sns.countplot(df_train[df_train['movieId'] == largest_std]['rating'])\n",
    "plt.title('Rating distribution for \"Santa with Muscles (1996)\"', y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"Santa with Muscles\" (a Christmas movie starring former wrestler Hulk Hogan) was the most polarising among reviewers, with the vast majority of votes being either 1.0 or 5.0. The polarizing score can either be interpreted as a terrible movie and that the 5.0 ratings are ironic/sarcastic ratings or that there are a lot of Hulk Hogan fans that use MovieLens and thus rate it well eventhough the movie itself is of low quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training a content based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we shall build is a content based model. This model makes recommendations to users based on the similarity between the movies which the users watched and other movies. It looks at things like similar genres, tags, keywords and title cast. This model is especially helpful for recommending movies to new users since no watching history of a user is required, only data about each movie. This method is however computationally very intensive since several pieces of information about each movie need to be taken into consideration before a recommendation can be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a test set free from data leakage, we will split the train set so that 25% of the data is not processed or trained on, but is kept solely for the purpose of performance assessment. We will also split between features and labels. Since this is unsupervised machine learning we will only be training our models using the features, but we will use the labels in the test set to help assess the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset between training data and testing data \n",
    "trainset, validationset = sklearn.model_selection.train_test_split(df_train[['userId', 'movieId', 'rating']], test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build this model by first weighing the importance of each word in the metadata column that we created earlier for this model. This column consists out of the tags and genres for each movie. We will weigh the importance of each word using the sklearn function TfidfVectorizer which will first remove all the English stop words, and then consider how many times a word occurs for a specific movie in comparison to all the movies in the dataset. \n",
    "We will then use the sklearn cosine_similarity function which converts the matrix created by the TfidfVectorizer function into a 2D array that shows the similarity of each movie with each other movie in the dataset.\n",
    "We will then use this similarity matrix to recommend the 10 best movies for a particular user after watching a particular movie and also predict a rating for each movie that a specific user will be watching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighing the significance of words with TfirdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let us start by running the TfidVectorizer on the metadata column created earlier.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df_final['metadata'])\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us just ensure a reduced so that it is not computationally too intensive\n",
    "tfidf_matrix = tfidf_matrix[:10000, :10000]\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a similarity matrix with cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let us now run the cosine_similarity function to work out the similarity between each movie with each other movie.\n",
    "cosine_sim_authTags = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print (cosine_sim_authTags.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommending movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the following function to work we will make the movie names the index and the index a column.\n",
    "indices = pd.Series(df_movies.index, index=df_movies['title'])\n",
    "indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now create a function that predicts the top N movies after a user watched a sepcific movie.\n",
    "def content_generate_top_N_recommendations(movie_title, N=10):\n",
    "    # Convert the string book title to a numeric index for our \n",
    "    # find the position of the book\n",
    "    b_idx = indices[movie_title]\n",
    "    \n",
    "    # Extracting all similarity values computed with the reference book title\n",
    "    sim_scores = list(enumerate(cosine_sim_authTags[b_idx]))\n",
    "    \n",
    "    # Sorting the values, keeping a copy of the original index of each value\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Selecting the top-N values for recommendation\n",
    "    sim_scores = sim_scores[1:N]\n",
    "    \n",
    "    # Collecting indexes \n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Converting the indexes back into titles \n",
    "    return df_movies.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create a test movie so that we can see if our function works\n",
    "test_movie = 'Toy Story (1995)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let us call the function created above with the test movie\n",
    "content_generate_top_N_recommendations(test_movie, N=10)[['movieId', 'title', 'rating', 'movie_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the content based model with Toy Story shows that other movies of similar content are suggested eventhough many of them received a poor rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting movie ratings for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us create a function that takes in a movie title and a user predicts the rating that user \n",
    "# would give to that movie.\n",
    "def content_generate_rating_estimate(movie_title, user, rating_data, k=20, threshold=0.0):\n",
    "    # Convert the book title to a numeric index for our \n",
    "    # similarity matrix\n",
    "    movie_index = indices[movie_title]\n",
    "    neighbors = [] # <-- Stores our collection of similarity values \n",
    "     \n",
    "    # Gather the similarity ratings between each book the user has rated\n",
    "    # and the reference book \n",
    "    for index, row in rating_data[rating_data['userId']==user].iterrows():\n",
    "        sim = cosine_sim_authTags[movie_index-1, indices[row['title']]-1]\n",
    "        neighbors.append((sim, row['rating']))\n",
    "    # Select the top-N values from our collection\n",
    "    k_neighbors = heapq.nlargest(k, neighbors, key=lambda t: t[0])\n",
    "\n",
    "    # Compute the weighted average using similarity scores and \n",
    "    # user item ratings. \n",
    "    simTotal, weightedSum = 0, 0\n",
    "    for (simScore, rating) in k_neighbors:\n",
    "        # Ensure that similarity ratings are above a given threshold\n",
    "        if (simScore > threshold):\n",
    "            simTotal += simScore\n",
    "            weightedSum += simScore * rating\n",
    "    try:\n",
    "        predictedRating = weightedSum / simTotal\n",
    "    except ZeroDivisionError:\n",
    "        # Cold-start problem - No ratings given by user. \n",
    "        # We use the average rating for the reference item as a proxy in this case \n",
    "        predictedRating = np.mean(rating_data[rating_data['title']==movie_title]['rating'])\n",
    "    return predictedRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In order for the function above to work we will put the movie title next to the userId, movieId and actual rating\n",
    "df_merged = pd.merge(df_train, df_movies[['movieId', 'title']], how = 'left', on = 'movieId')\n",
    "df_merged.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us this time create a test userId to see if our function works\n",
    "test_userId = 5152\n",
    "test_movie = \"Toy Story (1995)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now test out our function with our test_movie and our test_userId\n",
    "actual_rating = df_train[(df_merged['userId'] == test_userId) & (df_merged['title'] == test_movie)]['rating'].values[0]\n",
    "pred_rating = content_generate_rating_estimate(movie_title = test_movie, user = test_userId, rating_data = df_merged)\n",
    "print (f\"Title - {test_movie}\")\n",
    "print (\"---\")\n",
    "print (f\"Actual rating: \\t\\t {actual_rating}\")\n",
    "print (f\"Predicted rating: \\t {pred_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us this time create a test userId to see if our function works\n",
    "test_userId = 5027\n",
    "test_movie = \"Waterworld (1995)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now test out our function with our test_movie and our test_userId\n",
    "actual_rating = df_train[(df_merged['userId'] == test_userId) & (df_merged['title'] == test_movie)]['rating'].values[0]\n",
    "pred_rating = content_generate_rating_estimate(movie_title = test_movie, user = test_userId, rating_data = df_merged)\n",
    "print (f\"Title - {test_movie}\")\n",
    "print (\"---\")\n",
    "print (f\"Actual rating: \\t\\t {actual_rating}\")\n",
    "print (f\"Predicted rating: \\t {pred_rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus see that our content based recommender predicts close to the actual rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training collaborative based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the content based model the collaborative based models does not look at how similar movies are to each other  in order to make a helpful recommendation, but looks at how similar users' preferences are to make a useful recommendation. It will compare each user with each other user's watch and rating histories to establish user similarities and will then recommend the movies that similar users enjoyed. The collaborative based models are generally less computationally intensive and more accurate in its predictions (since it is based on similar preferences and not similar content), but suffers from the cold start problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build all of our models using the surprise package. This package was developed to help with the importing and splitting of data, building and training of various unsupervised machine learning models as well as to assess the accuracy of the models by comparing the predictions with the actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our collaborative models to train in a reasonable amount of time we will limit the rows to 10 000. This will unfortunately have the downside of predicting less accurately due to less data to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the shape of df_train\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not let us explore the shape of our ratings dataset after we have filtered it\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to reduce the training data from 10 million rows to 10 thousand rows. This is much easier for our machines to compute without running into memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SVD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD model starts by reducing the dimensions of the dataset so works normally quite well with sparse data. SVD decomposes any matrix into singular vectors and singular values. It then breaks up the data into three matrices and then extracts the factors by factorization. This is the model that won the $1,000,000 Netflix prize in 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Instantiating a reader and reading in our rating data\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_train[['userId','movieId','rating']], reader)\n",
    "\n",
    "# Splitting the train set to seperate a test set\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Building and fitting the model\n",
    "algorithm = SVD(random_state=42)\n",
    "algorithm.fit(trainset)\n",
    "\n",
    "# Making predictions on the testset\n",
    "predictions = algorithm.test(testset)\n",
    "\n",
    "# Testing for accuracy using Root Mean Square Error\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoClustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works by clustering high dimensional data such as what we have into clusters involving more than one dimension. In a 2 dimensional space we could explain it as clusters of blocks where both the row and the column affects the clustering block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and fitting the model\n",
    "algorithm2 = CoClustering(random_state=42)\n",
    "algorithm2.fit(trainset)\n",
    "\n",
    "# Making predictions on the testset\n",
    "predictions2 = algorithm2.test(testset)\n",
    "\n",
    "# Testing for accuracy using Root Mean Square Error\n",
    "accuracy.rmse(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The KNNBaseine model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNNBaseline algorithm is an unsupervised machine learning technique directly inspired by the k-nearest neighbors supervised machine learning algorithm. The KNN model is a non-parametric learning algorithm which takes almost no time to learn because it only stores the data of the training part and does not need to first learn a function. It clusters based on obsevations that are closest together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline\n",
    "\n",
    "# Building and fitting the model\n",
    "algorithm3 = KNNBaseline(random_state=42)\n",
    "algorithm3.fit(trainset)\n",
    "\n",
    "# Making predictions on the testset\n",
    "predictions3 = algorithm3.test(testset)\n",
    "\n",
    "# Testing for accuracy using Root Mean Square Error\n",
    "accuracy.rmse(predictions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hypertuned SVD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypertuned SVD model is the same model as our first model, but with hyperparameters set to what we think will cause it to perform best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a reader and reading in our rating data\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "data = Dataset.load_from_df(df_train[['userId','movieId','rating']], reader)\n",
    "\n",
    "# Splitting the train set to seperate a test set\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# Building and fitting the model\n",
    "algorithm = SVD(n_factors=1500, lr_all=0.006,reg_all=0.04,n_epochs=40,init_std_dev=0.05, random_state=42)\n",
    "algorithm.fit(trainset)\n",
    "\n",
    "# Making predictions on the testset\n",
    "predictions = algorithm.test(testset)\n",
    "\n",
    "# Testing for accuracy using Root Mean Square Error\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the RMSE that each trained model produced, we can conclude that the results of the 4 models are very similar, although the SVD performed the best. We will thus hand in the predictions produced by this model unto the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting predictions to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning of this notebook, this recommendation model forms part of a public Kaggle competition. Thus our trained model needs to predict on their external datasource and the prediction need to be handed in in their prescribed format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the test set we have to make predictions on and display it's head\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that only userId and movieId are given and that ratings should be estimated to match these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now predict on the trainset using our trained SVD model\n",
    "\n",
    "# This will take a while so please be patient running it :)\n",
    "\n",
    "def predict_rating(row):\n",
    "    u = row[\"userId\"]\n",
    "    i = row[\"movieId\"]\n",
    "    return algorithm.estimate(u, i)\n",
    "\n",
    "df_test_with_ratings = df_test.assign(rating=df_test.apply(predict_rating, axis=1))\n",
    "df_test_with_ratings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Our model predicted a rating for each userId and movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now see the prescribed hand-in format\n",
    "df_sample = pd.read_csv('sample_submission.csv')\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that the sample contains only an Id column consisting of the userId concatenated with the moveieId and the rating that our model should estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating each userId and movieId to a single Id column for submission\n",
    "df_test_with_ratings['Id'] =  df_test_with_ratings['userId'].astype(str).str.zfill(1) + '_' + df_test_with_ratings['movieId'].astype(str).str.zfill(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test.assign(Id=test.userId.astype(str)+\"_\"+test.movieId.astype(str))\n",
    "submission = df_test_with_ratings[[\"Id\", \"rating\"]]\n",
    "submission.to_csv(\"svd_h_tuned_submission_rec.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we set forth the process we followed to build and train a content based recommender and several collaborative based recommender models. For the content based model we first had to do extensive feature engineering and then reduce the dataset before we could train our model. For the collaborative based models we could simply build and train our models. In each case we had to split the data into a train and test set so that the models would not suffer from data leakage and overfitting. We also did extensive EDA to understand the data we are working with and to verify our conclusions with intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference list\n",
    "Although we constantly researched and used various sources to assist us with this notebook, we more heavily leaned on the following sources and are greatly indebted to them.\n",
    "* Explore Trains: Introduction_to_Recommender_Systems\n",
    "* Mills, P.(2017, October, 5). Singular Value Decomposition (SVD) Tutorial: Applications, Examples, Exercises. A complete tutorial on the singular value decomposition method. Retrieved from https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254\n",
    "* Becker, N.(2016, November, 10). Matrix Factorization for Movie Recommendations in Python. Retrieved from https://beckernick.github.io/matrix-factorization-recommender/\n",
    "* Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
    "* Kane, F.(n.d.) - Building Recommender Systems with Machine Learning and AI. Course available on www.udemy.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "935px",
    "left": "38.9062px",
    "top": "110.573px",
    "width": "300.677px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
